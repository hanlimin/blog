---
title: 可用性
date: "2020-12-10 23:10:25"
modifyDate: "2020-12-10 23:10:25"
draft: true
---
### 复制

参与复制的Redis实例划分为主节点（master）和从节点（slave）。默认情况下Redis都是主节点。每个从节点只能有一个主节点，而主节点可以同时具有多个从节点。复制的数据流是单向的，只能由主节点复制到从节点。

复制积压缓冲区是保存在主节点上的一个固定长度的队列，默认大小为1MB，当主节点有连接的从节点（slave）时创建，这时主节点响应写命令时，不大会把命令发送给从节点，还会写入复制积压缓冲区，实现保存最近已复制数据的功能，用于部分复制和复制命令丢失的数据补救。

每个Redis节点启动都会动态分配一个40位的十六进制字符串作为运行ID。运行ID的主要作用是唯一标识Redis节点，比如从节点保存主节点的运行ID识别自己正在复制的是哪个节点，因此当运行ID变化后从节点将做全量复制。

主从节点在建立复制后，它们之间维护者长连接并彼此发送心跳命令。主节点默认每隔10秒对从节点发送ping命令。从节点在主线程每隔1秒发送 replconf ack {offset}命令，给主节点上报自身当前的复制偏移量。

#### 配置

配置复制的方式有以下三种：

1）配置文件中加入slaveeof {masterHost} {masterPort}随Redis启动生效。

2）在redis-server启动命令后加入 --slaveeof {masterHost} {masterPort}生效。

3）直接使用命令：slaveeof {masterHost} {masterPort}生效。

#### 复制过程

1）保存主节点信息

2）主从建立socket连接

3）发送ping命令

4）权限验证

5）同步数据集

6）命令持续复制

### Redis Sentinel

在Redis 2.8版本开始正式提供了Redis Sentinel哨兵来完成故障发现和故障转移，实现高可用。

Redis Sentinel是个分布式架构，其中包含若干个Sentinel节点和Redis数据节点，每个Sentinel节点会对数据节点和其余Sentinel节点进行监控，当它发现节点不可达时，会节点做下线标识。如果被标识的是主节点，它还会和其它Sentinel节点进行协商，当大多数Sentinel节点认为主节点不可达，它们会选举一个Sentinel节点来完成自动故障转移工作，同时会将这个变化实时通知给Redis应用方。

#### 配置

```properties
port 26379
daemonize yes
# 需监控节点，不可达判断票数
sentinel monitor master 172.22.0.2 6379 2
# ping命令时间间隔
sentinel down-after-milliseconds master 30000
# 每次向新的节点发起复制操作的从节点个数
sentinel parallel-syncs master 1
# 故障转移超时时间0
sentinel failover-timeout master 180000
```

#### 三个定时任务

1. 每隔10秒，每个Sentinel节点会向主节点和从节点发送info命令获取最新的拓扑结构。
2. 每隔2秒，每隔Sentinel节点会向Redis数据节点的__sentinel\_\_:hello频道上发送该Sentinel节点对主节点的判断以及当前Sentinel节点的信息，同时每个Sentinel节点也会订阅该频道，来了解其它Sentinel节点以及它们对主节点的判断。
3. 每隔1秒，每个Sentinel节点会向主节点、从节点、其余Sentinel节点发送一条ping命令做一次心跳检测，来确认这些节点当前是否可达。

#### 主观下线和客观下线

每个Sentinel节点会每隔1秒发送ping命令做心跳检测，当这些节点超过down-after-milliseconds没有进行有效回复，Sentinel节点就会对该节点失败判断，这个行为叫做主观下线。

当Sentinel主观下线的节点是主节点时，该Sentinel节点会通过 sentinel  is-master-down-by-addr命令向其它Sentinel节点询问对主节点的判断，当超过<quorum>个数，Sentinel节点认为主节点有问题，这时该Sentinel节点就会做出客观下线的决定。

#### 领导者sentinel节点选举

故障转移的工作是由一个Sentinel节点负责的，所以要通过选举来选出一个Sentinel节点作为领导者进行故障转移的工作。Redis使用了Raft算法（ 一致性算法 ）实现领导者选择。

#### 故障转移

选举出的领导者Sentinel负责故障转移，具体步骤如下：

1）在从节点列表中选出一个一个节点作为新的主节点，选择方法如下

- 过滤：5秒内没有没有回复Sentinel节点的ping响应、与主节点失联超过down-after-milliseconds*10秒。
- 选择slave-priority最高的从节点列表，如果存在则返回，不存在继续。
- 选择复制偏移量最大的从节点，如果存在则返回，不存在继续。
- 选择runid最小的从节点。

2）Sentinel领导者会对第一选出来的从节点执行slaveof no one命令让其成为主节点。

3）Sentinel领导者会向剩余的从节点发送命令，让它们从未新主节点的从节点，复制规则和parallel-syncs参数有关。

4）Sentinel节点集合会将原来主节点更新为从节点，并保持着对其关注，当其恢复后命令它去复制新的主节点。

#### 节点维护

##### 节点下线

- 主节点

    使用sentinel failover命令使从节点晋身为主节点

- 从节点和Sentinel

    如果使用了读写分离，下线从节点要保证应用方可以感知到从节点下线，从而把读请求路由到其它节点。需要注意的使Sentinel节点依然会对着这些下线的节点进行定期监控。

##### 节点上线

- 添加从节点

    使用 slaveof命令

- 添加Sentinel节点

    添加setntinel moitor节点

- 添加主节点

    因为Redis Sentinel只能有一个主节点，所有不需要添加主节点，如果需要替换主节点，可以使用sentinel failover手动故障转移。

### Redis Cluster

Redis Cluster是Redis的分布式解决方案，在3.0版本正式推出，有效地解决了Redis分布式方面的需求。当遇到单机内存、并发、流量等瓶颈是，可以采用Cluster架构方案达到负载均衡的目的。

#### 数据分布

##### 数据分布理论

分布式数据库首先要解决把整个数据集按照分区规则映射到多个节点的问题，即把数据集分布到多个节点上，每个节点负责整体数据数据的一个子集。需要注意的数据的分区规则，常见的分区规则有哈希分区和顺序分区两种。

| 分区方式 | 特定                                           | 代表产品                             |
| -------- | ---------------------------------------------- | ------------------------------------ |
| 哈希分区 | 离散度好<br/>数据分布业务无关<br>无法顺序访问  | Redis Cluster<br>Cassandra<br>Dynamo |
| 顺序分区 | 离散度易倾斜<br>数据分布业务相关<br>可顺序访问 | Bigtable<br>HBase<br>Hypertable      |

###### 节点取余分区

使用特定的数据，如Redis的键或用户ID，再根据节点数量N使用工具：hash(key) % N计算出哈希值，用来决定数据映射到哪个节点区。这种方案存在一个问题：当节点数量变化时，如扩容或收缩节点，数据节点映射关系需要重新计算，会导致数据的重新迁移。

这种方式的突出优点是简单性，常用于数据库的分库分表规则，一般采用预分区的方式，提前根据数据量规划好分区数，保证可支撑未来一段时间的数据量，再根据负载情况将表迁移到其它数据库中。扩容时通常采用翻倍扩容，避免数据映射全部被打乱导致全量迁移的情况。

###### 一致性哈希分区

一致性哈希分区实现思路是为系统中每个节点分配一个token，范围一般在0 ~ $2^{32}$，这些token构成一个哈希环。数据读写执行节点查找操作时，先根据key计算hash值，然后顺时针找到第一个大于等于该哈希值的token节点。

这种方式相比节点取余最大好处在于加入和删除节点只影响哈希环相邻的节点，对于其它节点无影响。但一致性哈希分区存在以下问题：

- 加减节点会造成哈希环中部分数据无法命中，需要手动处理或忽视这部分数据，因此一致性哈希常用于缓存场景。
- 当使用少量节点时，节点变化将大范围影响哈希环中的数据映射，因此这种方式不适合少量数据节点的分布式方案。
- 普通的一致性哈希分区在增减节点时需要增加一倍或减去一倍节点才能保证数据和负载的均衡。

正因为一致性哈希分区的这些缺点，一些分布式系统采用虚拟槽对一致性哈希进行改进。

###### 虚拟槽分区

虚拟槽分区巧妙地使用了哈希空间，使用分散度良好的哈希函数把所有数据映射到一个固定范围的整数集合中，整数定义为槽（slot）。这个范围一般远远大于节点数，每个节点会负责一定数量的槽。槽是集群内数据管理和迁移的基本单位。采用大范围槽的主要目的是为方便数据拆分和集群扩张。

#### Redis数据分区

Redis Cluser采用虚拟槽分区，所有的键根据哈希函数映射到0 ~ 16383整数槽内，计算公式：slot=CRC16(key)&16383。每个系欸但负责维护一部分槽以及槽所映射的键值数据。Reidis虚拟槽分区的特点：

- 解耦数据和节点之间的关系，简化了节点扩容和收缩的难度。
- 节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据。
- 支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景。

#### 集群功能限制

Redis集群相对单机在功能上存在一些限制，在使用时做好规避。限制如下：

1. key批量操作支持有限。如mset、mget，目前只支持具有相同slot值得key执行批量操作。
2. key事务操作支持有限。只支持多key在同一节点上的事务操作。
3. key作为数据分区的最小颗粒，因此不能将一个大的键值对象如hash、list等映射到不同的节点。
4. 不知多数据库空间。单机下的Redis可以支持16个数据库，集群模式下只能使用一个数据库空间。
5. 复制结构只支持一层，从节点之能复制主节点，不支持嵌套树状复制结构。

#### 搭建集群

搭建集群的工作需要以下三个步骤：

1）准备节点。

2）节点握手。

3）分配槽。

##### 准备节点

添加相应配置

```properties
# 开启集群模式
cluster-enable yes
# 节点超时时间，单位毫秒
cluster-node-timeout 15000
# 集群内部配置文件
cluster-config-file "chuster.conf"
```

集群模式的Redis除了原有的配置文件之外又加了一份集群配置文件内。当集群内节点信息发生变化，如添加节点、节点下线、故障转移等，节点会自动保存集群状态到配置文件中。需要注意的是，Redis自动维护集群配置文件，不要手动修改，防止节点重启时产生集群信息错乱。

##### 节点握手

节点握手是指一批运行在集群下的节通过Gossip协议彼此通信，达到感知对方的过程。节点握手是集群彼此通信的第一步，在任意节点执行cluster meet <ip> <port>完成握手，主要作用是节点之间交换状态数据信息，使节点加入集群中，但这时集群处于下线状态。

##### 分配槽

Redis集群把所有的数据映射到16384个槽中。每个key会映射为一个固定的槽，只用当节点分配了槽，才会响应和这些槽相关联的命令。通过cluster addslotsml为节点分配槽。完成分配槽后，集群进入在线状态。在集群模式下，Redis节点角色分为主节点和从节点。首次启动的节点和被分配槽的节点都是主节点，从节点负责复制主节点槽信息和相关的数据。使用cluster replication <nodeId>命令让一个节点成为从节点。其中命令执行必须在对应的从节点行执行，nodeId是要复制主节点的节点ID。

可以使用redis-trib.rb工具来简化集群创建操作

#### 节点通信

##### 通信流程

在分布式储存中需要提供维护节点元数据信息的机制，所谓元数据是指：节点负责哪些数据，是否出现故障等状态信息。常见的元数据维护方式为：集中式和P2P方式。Redis集群采用P2P的Gossip协议，Gossip协议工作原理就是节点彼此不断通信交换信息，一段时间后所有的节点都会知道集群的完整信息，这种方式类似流言传播。通信过程说明：

1）集群中的每个节点都会单独开辟一个TCP通道，用于节点之间彼此通信，通信端口号在基础端口加上10000。

2）每个节点在固定周期内通过特定规则选择几个节点发送ping信息。

3）接收到ping消息的节点用pong消息作为响应。

集群中每个节点通过一定规则挑选要通信的节点，每个节点可能知道全部节点，也可能仅知道部分节点，之后这些节点彼此可以正常通信，最终它们会达到一致的状态。当节点故障、新节点加入、主从角色变化、槽信息变更等事件发生时，通过不断的ping/pong消息，经过一段时间后所有的节点都会知道整个集群全部节点的最新状态，从而达到集群状态同步的目的。

##### Gossip消息

Gossip协议的主要职责就是信息交换。信息交换的载体就是节点彼此发送的Gossip信息，集群中常用的Gossip消息可分为：ping消息、pong消息、meet消息、fail消息等。

- meet消息：用于通知新节点加入。消息发送者通知接收者加入到当前集群中，meet消息通信正常完成后，接受者节点会加入到集群中并进行周期性的ping、pong消息交换。
- ping消息：集群内交换最频繁的消息，集群内每个阶段每秒向多个其它节点 发送ping消息，用于检测节点是否在线和交换彼此状态信息。ping信息发送封装了自身节点和部分其它节点的状态消息。
- pong消息：当节点收到ping、meet消息时，作为响应消息回复给发送方确认信息正常通信。pong信息内部封装了自身状态数据。节点也可向集群内广播自身的pong信息来通知整个集群对自身状态进行更新。
- fail消息：当节点判断集群内另一个节点下线时，会向集群内广播一个fail消息，其它节点接收到fail消息之后会把对应节点更新为下线。

#### 扩容集群

扩容是分布式储存最常见的需求，Redis集群扩容操作如下步骤：

1）准备新节点。

2）加入集群。

3）迁移槽和数据

（1）槽迁移计划

​	槽是Redis集群管理数据的基本单元，首先需要为新节点定制槽的迁移计划，确定原有节点的哪些槽要需要迁移到新节点。

（2）迁移数据

​	数据迁移的过程是逐个槽进行的，流程说明：

1. 对目标节点发送 cluser setslot <slot> importing <sourceNodeId>命令，令目标节点准备导入槽的数据。
2. 对源节点发送cluster setslot <slot> migrating <targetNodeId>命令，让源节点准备迁出槽的数据。
3. 源节点循环执行cluster getkeysinslot <slot> <count>命令，获取count个属于<slot>的键。
4. 在源节点上执行migrate <targetIp> <targetPort> "" 0 <timeout> keys <keys ... >命令，把获取的键通过流水线(pipeline)机制批量迁移到目标节点，批量迁移版本的migrate命令在Redis 3.0.6以上提供，之前的migrate命令只能单个键迁移。对于大量key的场景，批量键迁移将极大降低节点之间网络IO次数。
5. 重复执行步骤3和步骤4直到槽下所有的键值数据迁移到目标节点。
6. 向集群中所有主节点发送cluster setslot <slot> node <targetNodeId>命令，通知槽分配给目标节点。为了保证槽节点映射变更及时传播，需要遍历发送给所有主节点更新被迁移的槽指向新节点。

实际操作时肯定涉及大量的槽并且每个槽对应非常多的键，可以使用迁移工具简化工作，比如redis-trib提供的槽重分片功能。

#### 收缩集群

收缩集群意味着缩减规模，需要从现有集群中安全下线部分节点。安全下线节点步骤如下：

1）首先需要确定下线节点是否有负责的槽，如果是，需要把槽迁移到其它节点，保证节点下线后整个集群槽节点映射的完整性。

2）当下线节点不在负责槽或者本身是从节点，就可以通知集群内其它节点忘记下线节点，当所有节点忘记该节点后可以正常关闭。可以通过cluster forget <downNodeId>命令忘记节点。

可以通过工具简化操作，如redis-trib.rb。

#### 请求路由

##### 请求重定向

在集群模式下，Redis接收任何键相关命令时首先计算键对应的槽，再根据槽找出所对应的节点，如果是本身，则处理键命令；否则回复MOVED重定向错误，通知客户端请求正确的地址。这个过程称为MOVED重定向。

其中，计算槽方法为：键的有效部分使用CRC16函数计算出散列值，再取对16383的余数，使每个键都可以映射到0 ~ 16383槽范围内。如果键内容包含 { 和 } 大括号字符，则计算槽的有小部分使括号内的内容，这部分内容又叫做hash_tag，它提供不同键可以具备相同slot的功能。

##### ASK重定向

Redis集群支持在线迁移槽和数据来完成水平伸缩，当slot对应的数据从源节点到目标节点迁移过程中，客户端需要做到只能识别，保证键命令可正常执行。例如当一个slot数据从源节点迁移到目标节点时，期间可能出现一部分数据在源节点，另一部分再目标节点。当出现上述情况时，客户端键命令执行流程发生变化，如下所示：

1）客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直接执行并返回结果给客户端。

2）如果键对象不存在，则可能在目标节点，这时源节点会回复ASK重定向异常。格式如下：（error） ASK  <slot> <targetIP>:<tragetPort>。

3）客户端从ASK重定向异常中提取目标节点信息，发送asking命令到目标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返回不存在信息。

ASK和MOVDED虽然都是对客户端的重定向控制，但是有本质的差别。ASK重定向说明集群正在slot数据迁移，客户端无法直到什么时候迁移完成，因此只能时临时性的重定向，客户端不会更新slots缓存。但是MOVED重定向说明键对应的槽已经明确指定新的节点，因此需要更新slots缓存。

#### 故障转移

##### 发现故障

Redis集群内通过节点通过ping/pong消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其它状态如：主从状态、节点故障等。因此故障发现也是通过消息传播机制实现的，主要环节包括：主观下线（pfail）和客观下线（fail）。

- 主观下线：

    指某个节点认为另一个节点不可用，即下线状态，这个状态不是最终的故障判断，只能代表一个节点的意见，可能存在误判的情况。

- 客观下线：

    指标记一个节点真正的下线，集群内多个节点都认为该节点不可用，从而达成共识的结果。如果是持有槽的主节点故障，需要为该节点进行故障转移。

##### 主观下线

集群中每个节点都会定期向其它节点发送ping下线，接受节点回复pong信息作为响应。如果在cluster-node-timeout时间内通信一直失败，则发送节点认为会认为接受节点存在故障，把接受节点标记为主观下线状态。

##### 客观下线

当某个节点判断另一个节点客观下线后，相应的节点状态会跟随消息在集群内传播。ping/pong消息的消息体会携带集群1/10的其它节点状态信息，当接受节点发现消息体中含有主观下线的节点状态时，会在本地找到故障节点的ClusterNode结构，保存到下线报告链表中。

通过Gossip消息传播，集群内节点不断收集到故障节点的下线报告。当半数以上持有槽的主节点都标记某节点下线时主观下线时，向集群广播一条fail消息，通知所有节点将故障节点标记为客观下线，fail消息的消息体只包含故障节点的ID。

每个下线报告都存在有效期，每次在尝试触发客观下线时，都会检测下下线报告是否过期，对于过期的报告将会被删除。过期时间为cluster-node-time*2。

##### 故障恢复

故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它的从节点中选出一个替换它，从而保证集群可用性。下线主节点的所有从节点承担着故障恢复的义务，当从节点通过内部定时任务发现自身复制的主节点进入客观下线时，将会触发恢复流程。

1. 资格检查

    每个从节点都要检查最后于主节点断线时间，判断是否有资格替换故障的主节点。如果从节点与主节点断线时间超过cluster-node-time*cluster-slave-validity-factor，则当前从节点不具备故障转移资格。参数cluster-slave-validity-factor用于从节点的有效因子，默认为10。

2. 准备选举时间

    当从节点符合故障转移资格后，更新触发故障选举的时间，只有达到该时间后才能执行后续流程。这里之所有采用延迟触发机制，主要是通过多个从节点使用不同的延迟选举时间来支持优先级问题。复制偏移量越大说明从节点延迟越低，那么它应该具有更高的优先级来替换故障主节点。

3. 发起选举

    当从节点定时任务检查到达故障选取时间（failover_auth_time）到达后，发起选取流程如下：

    1. 更新配置纪元

        配置纪元是一个只增不减的整数，每个主节点自身维护一个配置纪元标示当前主节点的版本，所有主节点的配置纪元都不相等，从节点会复制主节点的配置纪元。整个集群有维护一个全局的配置纪元，用于记录集群内所有节点配置纪元的最大版本。通过cluster info命令可用查看配置纪元信息。

    2. 广播选举信息

        在集群内广播选举信息（FAILOVER_AUTH_REQUEST），并记录已发送消息的状态，保证该从节点在一个配置纪元内只能发送一次选举。

4. 选举投票

    只有持有槽的主节点才会处理故障选举信息（FAILOVER_AUTH_REQUEST），因为每个持有槽的节点在一个配置纪元都有唯一的一张选票，当接到第一个请求投票的从节点信息时回复FAILOVER_AUTH_ACK消息作为投票，之后相同配置纪元内其它从节点的选举消息将忽略。

    投票过程其实是个领导者选举过程，如集群内有N个持有槽的主节点代表有N张票。由于在每个配置纪元内持有槽的主节点只能投票给一个从节点，因此只能有一个从节点获取的N/2+1的选票，保证能够找出唯一的从节点。

5. 替换主节点

    当从节点收集到足够的选票后，触发替换主节点操作：

    1）当前从节点取消复制变为主节点。

    2）执行clusterDelSlot操作撤销故障主节点负责的槽，并执行clusterAddSlot把这些槽委派给自己。

    3）向集群广播自己的pong消息，通知集群内所有节点当前从节点变为主节点并接管了故障主节点的槽信息。

##### 故障转移的时间

1）主观下线识别时间=cluster-node-timeout。

2）主观下线状态信息传播时间<=cluster-node-timeout/2.

3）从节点转移时间<=1000毫秒。

根据以上分析可预估出故障转移时间如下：

failover-time <= cluster-node-timeout +  cluster-node-timeout/2 + 1000

因此，故障转移时间跟cluster-node-timeout参数息息相关，默认15秒。配置时间可以根据业务容忍度做适当调整，当不是越小越好。

#### 集群读写分离

##### 只读连接

集群模式下从节点从不接受任何读写请求，发送过来的键命令会重定向到负载槽的主节点上。当需要使用从节点分担主节点度压力时，可以使用readonly命令打开客户端连接只读状态。之前的复制配置slave-read-only在集群模式下无效。当开启只读模式时，从节点接受到读取命令处理流程变为：如果对应的槽属于自己正在复制的主节点则直接执行读命令，否则返回重定向命令。

##### 读写分离

集群模式下的读写分离，同样会遇到：数据延迟、读到过期数据、从节点故障等问题。集群模式下读写分离成本较高，可以直接扩展主节点数量提高集群性能，一般不建议集群模式下做读写分离。
